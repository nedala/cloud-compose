FROM nvidia/cuda:11.0.3-runtime-ubi8

USER root

# Add ZScaler Certs
COPY certificates/*.crt /etc/pki/ca-trust/source/anchors/
COPY certificates/*.crt /etc/ssl/certs/
RUN cat /etc/ssl/certs/*.crt >> /etc/ssl/certs/cbp_bundle.crt
ENV SSL_CERT_DIR=/etc/ssl/certs/
ENV SSL_CERT_FILE=/etc/ssl/certs/cbp_bundle.crt
ENV REQUESTS_CA_BUNDLE=/etc/ssl/certs/cbp_bundle.crt
RUN update-ca-trust

# Install curl and wget
RUN dnf install -y \
    wget \
    curl \
    git \
    unzip \
    java-11-openjdk.x86_64 \
    mesa-libGL \
    make automake gcc gcc-c++ graphviz cmake clang llvm-devel \
    gcc zlib-devel bzip2 bzip2-devel sqlite sqlite-devel openssl-devel xz xz-devel libffi-devel \
    ca-certificates \
    nano vim zip unzip npm && \
    dnf clean all

# Install ODBC
ENV ACCEPT_EULA=Y
RUN curl https://packages.microsoft.com/keys/microsoft.asc  | gpg --import -
RUN curl https://packages.microsoft.com/config/rhel/8/prod.repo > /etc/yum.repos.d/mssql-release.repo && \
    dnf install -y msodbcsql18 mssql-tools18 unixODBC-devel && \
    dnf clean all

# Install Conda
WORKDIR /opt
ENV JAVA_HOME=/etc/alternatives/jre_openjdk

# Add s3a bindings
ENV AWS_JAR=1.11.901

# Install Hadoop
ENV HADOOP_VERSION=3.2.3
ENV HADOOP_HOME=/opt/hadoop
WORKDIR ${HADOOP_HOME}
RUN curl -sSL https://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz | \
    tar -xz -C ${HADOOP_HOME} --strip-components 1

# Install Apache Hive
ENV HIVE_VERSION=3.1.3
ENV HIVE_HOME=/opt/hive
WORKDIR ${HIVE_HOME}
RUN curl -sSL https://repo1.maven.org/maven2/org/apache/hive/hive-standalone-metastore/${HIVE_VERSION}/hive-standalone-metastore-${HIVE_VERSION}-bin.tar.gz | \
    tar -xz -C ${HIVE_HOME} --strip-components 1

# Install Spark
ENV SPARK_HOME=/opt/spark
ENV SPARK_VERSION=3.4.1
WORKDIR ${SPARK_HOME}
RUN curl -sSL https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz | \
        tar -xz -C ${SPARK_HOME} --strip-components 1

# Install Flume
ENV FLUME_HOME=/opt/flume
ENV FLUME_VERSION=1.11.0
WORKDIR ${FLUME_HOME}
RUN curl -sSL https://dlcdn.apache.org/flume/${FLUME_VERSION}/apache-flume-${FLUME_VERSION}-bin.tar.gz | \
        tar -xz -C ${FLUME_HOME} --strip-components 1

# Install Minio
ENV MINIO_HOME=/opt/minio
WORKDIR ${MINIO_HOME}
RUN wget https://dl.minio.io/client/mc/release/linux-amd64/mc -O ${MINIO_HOME}/mc && \
    chmod a+x ${MINIO_HOME}/mc

# Install AWS Minio libs
ENV JAR_HOME=/opt/jars
WORKDIR ${JAR_HOME}
COPY jars/*.jar ${JAR_HOME}/
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar -O ${JAR_HOME}/hadoop-aws-${HADOOP_VERSION}.jar
RUN wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAR}/aws-java-sdk-bundle-${AWS_JAR}.jar -O ${JAR_HOME}/aws-java-sdk-bundle.jar
RUN curl -sSL https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-j-8.0.33.tar.gz | \
    tar -xz -C ${JAR_HOME} --strip-components 1
RUN rm -f ${HIVE_HOME}/lib/guava-*.jar ${SPARK_HOME}/jars/guava*.jar ${FLUME_HOME}/lib/guava*.jar ${HIVE_HOME}/lib/guava*.jar && \
    cp ${JAR_HOME}/*.jar ${HADOOP_HOME}/share/hadoop/common/lib/ && \
    cp ${JAR_HOME}/*.jar ${SPARK_HOME}/jars/ && \
    cp ${JAR_HOME}/*.jar ${FLUME_HOME}/lib/ && \
    cp ${JAR_HOME}/*.jar ${HIVE_HOME}/lib/

ENV PYTHON_HOME="/root/miniconda3"
ENV PATH=${PATH}:${SPARK_HOME}/bin:${JAVA_HOME}/bin:${MINIO_HOME}/:${HADOOP_HOME}/bin:${HIVE_HOME}/bin:${FLUME_HOME}/bin:${PYTHON_HOME}/bin

RUN wget https://repo.anaconda.com/miniconda/Miniconda3-py39_23.3.1-0-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-py39_23.3.1-0-Linux-x86_64.sh -b \
    && rm -f Miniconda3-py39_23.3.1-0-Linux-x86_64.sh

# Install Jupyter Env with CUDA Enabled
ADD conda_packages.txt .
RUN conda install --file conda_packages.txt
ENV PIP_CERT=${REQUESTS_CA_BUNDLE}
RUN pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.12.* pandas numba openpyxl xlsxwriter requests black isort yapf
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117

# Install Requirements.txt
ADD requirements.txt .
RUN pip install --upgrade pip && pip install -r requirements.txt

# Addl dependencies
WORKDIR /tmp
ADD addl_requirements.txt .
RUN pip install -r addl_requirements.txt


# Add Delta Package
RUN echo :quit | ${SPARK_HOME}/bin/spark-shell --packages io.delta:delta-core_2.12:2.4.0 && \
    find /root/.ivy2/ -name "*.jar" | xargs -I{} cp {} ${SPARK_HOME}/jars/

# Ports Spark + Jupyter + Airflow + MLFlow
EXPOSE 4040
EXPOSE 4041
EXPOSE 8888
EXPOSE 8080
EXPOSE 5000
EXPOSE 5001
EXPOSE 4001
EXPOSE 9000
EXPOSE 9083
EXPOSE 10000

# Patch pip
RUN rm -Rf /root/.cache/pip

# Install trino client
RUN wget https://repo1.maven.org/maven2/io/trino/trino-cli/420/trino-cli-420-executable.jar -O ${JAR_HOME}/trino.jar && \
    alias trino="java -jar ${JAR_HOME}/trino.jar --server http://trino:8080 --catalog minio" 

# Patch paths for Tensorflow in Miniconda
ENV CONDA_PREFIX=/root/miniconda3
ENV CUDNN_PATH=${CONDA_PREFIX}/lib/python3.9/site-packages/nvidia/cudnn
ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib

WORKDIR /jupyter
# Add CMD to auto start jupyter lab
COPY entrypoint.sh /entrypoint.sh
RUN chmod a+x /entrypoint.sh
CMD ["/entrypoint.sh"]
